{
 "metadata": {
  "name": "",
  "signature": "sha256:4fb85b47ff9dbb98d802dcfdc5e079b734abeba2eb536426fd73a59110609a18"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%javascript\n",
      "function is_local(){\n",
      "  return (document.location.hostname == \"localhost\" || document.location.hostname == '127.0.0.1')\n",
      "}\n",
      "var url = is_local() ? \"http://localhost:8000/theme/custom.js\" : \"http://odhk.github.io/hyrule_theme/custom.js\"\n",
      "$.getScript(url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "javascript": [
        "function is_local(){\n",
        "  return (document.location.hostname == \"localhost\" || document.location.hostname == '127.0.0.1')\n",
        "}\n",
        "var url = is_local() ? \"http://localhost:8000/theme/custom.js\" : \"http://odhk.github.io/hyrule_theme/custom.js\"\n",
        "$.getScript(url)"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Javascript at 0x3792f10>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Nature is a self-made machine, more perfectly automated than any automated machine. To create something in the image of nature is to create a machine, and it was by learning the inner working of nature that man became a builder of machines\n",
      "\n",
      "<footer>~ Eric Hoffer</footer>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/agenda.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Discriminative & Generative Algorithm\n",
      "1. Support Vector Machines\n",
      "1. Maximum Margin Hyperplanes\n",
      "1. Slack Variables\n",
      "1. Non-linear SVM Classification\n",
      "\n",
      "** Labs **\n",
      "1. Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/theory.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Discriminative & Generative Algorithm"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Generative: P(x|y), P(y)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Deeper understanding: modeling classes. Can generate data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$f(x) = arg \\max  _{y}p(x|y), p(y)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which is the equation you use in generative models. It has the joint probability distribution $p(x, y)$, since $p(x, y) = p(x | y) p(y)$, which explicitly models the actual distribution of each class. With the joint probability distribution function, given an $y$, you can calculate (\"generate\") its respective $x$. For this reason they are called generative models."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A generative model learns the joint probability distribution $p(x,y)$ and a discriminative model learns the conditional probability distribution $p(y|x)$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Discriminative: P(y|x)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pragmatic: focuses on distinguishing. Can only classify"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$f(x) = arg \\max  _{y}p(y|x)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which merely chooses what is the most likely class considering x. Here we have the conditional probability distribution p(y|x), which modeled the boundary between classes,"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's a really simple example. Suppose you have the following data in the form $(x,y)$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = [(1,0), (1,0), (2,0), (2, 1)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$p(x,y)$ is"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "          y=0   y=1\n",
      "         -----------\n",
      "    x=1 | 1/2   0\n",
      "    x=2 | 1/4   1/4"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$(y|x)$ is\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "          y=0   y=1\n",
      "         -----------\n",
      "    x=1 | 1     0\n",
      "    x=2 | 1/2   1/2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classifying Methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **Logistic Regression**: Discriminative\n",
      "* **KNN**: Generative\n",
      "* **Na\u00edve Bayes**: Generative\n",
      "* **Decision Trees**: Discriminative\n",
      "* **SVM**, a non-probabilistic maximal margin classifier ?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/resources.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Journal Articles"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes](http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf)\n",
      "* [Learning Generative Models via Discriminative Approaches](http://pages.ucsd.edu/~ztu/publication/cvpr07_gdl.pdf)\n",
      "* [Discriminative Learning can Succeed where Generative Learning Fails](http://www.cs.columbia.edu/~rocco/Public/ipl_lss.pdf)\n",
      "* [Generative or Discriminative? Getting the Best of Both Worlds](http://research.microsoft.com/en-us/um/people/cmbishop/downloads/bishop-valencia-07.pdf)\n",
      "* [Discriminative, Generative and Imitative Learning (PhD Thesis)](http://www.cs.columbia.edu/~jebara/papers/jebara4.pdf)\n",
      "* [Generative vs. discriminative](http://stats.stackexchange.com/questions/12421/generative-vs-discriminative)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Video Lecture"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Generative Learning Algoritms](http://openclassroom.stanford.edu/MainFolder/VideoPage.php?course=MachineLearning&video=06.1-NaiveBayes-GenerativeLearningAlgorithms&speed=100)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/theory.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A binary linear classifier whose decision boundary is explicitly constructed to minimize generalization error."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* binary classifier \u2013 solves two-class problem\n",
      "* linear classifier \u2013 creates linear decision boundary (in 2d)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The decision boundary derived using geometric reasoning.\n",
      "In 2 dimensions, the separator is a line:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$y = ax + b \\\\ w_{1}x_{1} + w_{2}x_{2} + w_0 = 0$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In 3 dimensions, it\u2019s a 2D plane:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + w_0 = 0$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The generalization error is equated with the geometric concept of margin, which is the region along the decision boundary that is free of data points."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Margins provide the largest impact: even moving one point along the margin can completely change the decision boundary!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/svm.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal of an SVM is to create the linear decision boundary with the largest margin. This is commonly called the maximum marginhyperplane."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hyperplane: is just a high-dimensional generalization of a line."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/theory.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Maximum Margin Hyperplanes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Decision boundary (mmh) derived by the discriminant function."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$f(x) = w^{T}x+b$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "such that $w$ is the weight vector and $b$ is the bias."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The weight vector determines the orientation of the decision boundary. The bias determines its translation from the origin."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/svm_hardboundaries.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The sign of $f(x)$ determines the (binary) class label of a record $x$ ."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we said before, SVM solves for the decision boundary that minimizes generalization error, or equivalently, that has the maximum margin."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are the same things, because using the mmh as the decision boundary minimizes the probability that a small perturbation in the position of a point produces a classification error."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intuitively, the wider the margin, the clearer the distinction between classes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Selecting the mmh is a straightforward exercise in analytic geometry (we won\u2019t go through the details here). In particular, this task reduces to the optimization of a convex objective function."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/svm_convex.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The black curve $f(x)$ is a convex function of $x$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is nice because convex optimization problems are guaranteed to give global optima (and they\u2019re easy to solve numerically too)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that the margin depends only on a subset of the training data; namely, those points that are nearest to the decision boundary. These points are called the *support vectors*. The other points (far from the decision boundary) don\u2019t affect the construction of the mmh at all!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Decision Boundaries"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All of the decision boundaries we\u2019ve seen so far have split the data perfectly; eg, the data are linearly separable, and therefore the training error is 0. We are always solving for one optimum (global, instead of the local optimum). If our data (like in previous examples) is linearly separable, the training error is 0. The optimization problem that this SVM solves is:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\begin{equation*}\n",
      "\\begin{aligned}\n",
      "& \\underset{w,b}{\\text{minimize}}\n",
      "& & \\frac{1}{2} || w ||^2 \\\\\n",
      "& \\text{subject to}\n",
      "& & y_{i}(w^{T}x_{i}+b) \\geq 1 \\quad i =1, \\dots , n.\n",
      "\\end{aligned}\n",
      "\\end{equation*}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This type of optimization problem is called a quadratic program. The result of this qp is the hard margin classifier we\u2019ve been discussing."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Slack Variables"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall that in building the hard margin classifier, we assumed that our data was linearly separable (eg, that we could perfectly classify each record with a linear decision boundary)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose that this was not true, or suppose that we wanted to use a larger margin at the expense of incurring some training error. This can be done using by introducing *slack variables*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/svm_softboundaries.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Slack variables $\u03be$ generalize the optimization problem to permit some misclassified training records (which come at a cost $C$ )."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The resulting *soft margin classifier* is given by:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\begin{equation*}\n",
      "\\begin{aligned}\n",
      "& \\underset{w,b}{\\text{minimize}}\n",
      "& & \\frac{1}{2} || w ||^2 + C \\sum\\limits_{i=1}^n \\xi_i \\\\\n",
      "& \\text{subject to}\n",
      "& & y_{i}(w^{T}x_{i}+b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 \\quad i =1, \\dots , n.\n",
      "\\end{aligned}\n",
      "\\end{equation*}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The soft-margin optimization problem can be rewritten as the dual formulation of the optimization problem. (reached via Lagrange multipliers)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\begin{equation*}\n",
      "\\begin{aligned}\n",
      "& \\underset{\\alpha}{\\text{maximize}}\n",
      "& & \\sum\\limits_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n y_{i}y_{j}\\alpha_{i}\\alpha_{j}x^{T}_{i}x_{j} \\\\  \n",
      "& \\text{subject to}\n",
      "& & \\sum\\limits_{i=1}^n y_{i}\\alpha_{i} = 0, \\quad 0 \\leq \\alpha_{i} \\leq C\n",
      "\\end{aligned}\n",
      "\\end{equation*}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Often, the optimization of slack variables comes between exponentially growing sequences of $C$ (cost)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Higher values of *C* = higher accuracy in model\n",
      "* Lower values of *C* = training error and better generalization"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Non-linear SVM Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we need a more complex classifier than a linear decision boundary allows. One possibility is to add nonlinear combinations of features to the data, and then to create a linear decision boundary in the enhanced (higher-dimensional) feature space. This linear decision boundary will be mapped to a nonlinear decision boundary in the original feature space."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/hyper-transform.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Issues:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Does not scale well: requires many high-dimensional calculations\n",
      "* Leads to more complexity (both modeling complexity and computational complexity) than we want."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let\u2019s hang on to the logic of the previous example, namely:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* remap the feature vectors $x_i$ into a higher-dimensional space $K\u2019$\n",
      "* create a linear decision boundary in $K\u2019$\n",
      "* back out the nonlinear decision boundary in $K$ from the result"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But if we want to save ourselves the trouble of doing a lot of additional high-dimensional calculations, we only need recall that our optimization problem depends on the features only through the inner product $x^{T}x$ :"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\begin{equation*}\n",
      "\\begin{aligned}\n",
      "& \\underset{\\alpha}{\\text{maximize}}\n",
      "& & \\sum\\limits_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n y_{i}y_{j}\\alpha_{i}\\alpha_{j}x^{T}_{i}x_{j} \\\\  \n",
      "& \\text{subject to}\n",
      "& & \\sum\\limits_{i=1}^n y_{i}\\alpha_{i} = 0, \\quad 0 \\leq \\alpha_{i} \\leq C\n",
      "\\end{aligned}\n",
      "\\end{equation*}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can replace this inner product with a more general function that has the same type of output as the inner product!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The Kernel Trick"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nonlinear applications of SVM rely on an implicit (nonlinear) mapping $\u03a6$ that sends vectors from the original feature space $K$ into a higher-dimensional feature space $K\u2019$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Non-linear classification in $K$ is then obtained by creating a linear decision boundary in $K$.   "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In practice, this involves no computations in the higher dimensional space!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/svm_mapping.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The inner product is an operation that takes two vectors and returns a real number. The fact that we we can rewrite the optimization problem in terms of the inner product means that we don\u2019t actually have to do any calculations in the feature space $K$ . In particular, we can easily change K to be some other space $K\u2019$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Formally, we can think of the inner product as a map that sends two vectors in the feature space $K$ into the real line $\\mathbb{R}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can replace this with a generalization of the inner product called a kernel function that maps two vectors in a higher-dimensional feature space $K\u2019$ into $\\mathbb{R}$. The upshot is that we can use a kernel function to implicitly train our model in a higher-dimensional feature space, without incurring additional computational complexity! As long as the kernel function satisfies certain conditions ([Mercer\u2019s theorem](http://en.wikipedia.org/wiki/Mercer's_theorem)), our conclusions above regarding the mmh continue to hold. In other words, no algorithmic changes are necessary, and all the benefits of a linear SVM are maintained."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning \u2018far\u2019 and high values meaning \u2018close\u2019. The $C$ parameter trades off misclassification of training examples against simplicity of the decision surface. A low $C$ makes the decision surface smooth, while a high $C$ aims at classifying all training examples correctly.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/popular-kernels.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/polunomials-kernels.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/gausian-kernels.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SVMs (and kernel methods in general) are versatile, powerful, and popular techniques that can produce accurate results for a wide array of classification problems. The main disadvantage of SVMs is the lack of intuition they produce. These models are truly black boxes!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/code.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Implementing Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Implementation of Support Vector Classification (SVC) examples\n",
      "* Working with different kernels, costs, and gamma\n",
      "* Support Vector Regressions: Does this work?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SVC Implementation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Iris set implementation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      " \n",
      "\n",
      "from sklearn import svm, datasets, metrics\n",
      "iris = datasets.load_iris()\n",
      "# SVC Support Vector Classifier\n",
      "classifier = svm.SVC().fit(iris.data, iris.target)\n",
      "classifier.predict(iris.data)\n",
      "\n",
      "print metrics.classification_report(classifier.predict(iris.data), iris.target)\n",
      "\n",
      "# get support vectors\n",
      "classifier.support_vectors_\n",
      "\n",
      "# get indices of support vecbtors\n",
      "classifier.support_\n",
      "\n",
      "# get number of support vectors for each class\n",
      "classifier.n_support_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Kernels"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`SVC()` standard uses the rbf kernel. Let's compare implementation of each and margins by plotting the performance of each."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To make plotting easier, let's just use the first two features.\n",
      "X = iris.data[:, :2]\n",
      "Y = iris.target\n",
      "h = .02  # step size in the mesh\n",
      "\n",
      "# we create an instance of SVM and fit out data. We do not scale our\n",
      "# data since we want to plot the support vectors\n",
      "C = 1.0  # SVM regularization parameter\n",
      "svc = svm.SVC(kernel='linear', C=C).fit(X, Y)\n",
      "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, Y)\n",
      "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, Y)\n",
      "lin_svc = svm.LinearSVC(C=C).fit(X, Y)\n",
      "\n",
      "# create a mesh to plot in\n",
      "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
      "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
      "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
      "                     np.arange(y_min, y_max, h))\n",
      "\n",
      "# title for the plots\n",
      "titles = ['SVC with linear kernel',\n",
      "          'SVC with RBF kernel',\n",
      "          'SVC with polynomial (degree 3) kernel',\n",
      "          'LinearSVC (linear kernel)']\n",
      "\n",
      "\n",
      "for i, clf in enumerate((svc, rbf_svc, poly_svc, lin_svc)):\n",
      "    # Plot the decision boundary. For that, we will assign a color to each\n",
      "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
      "    plt.subplot(2, 2, i + 1)\n",
      "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "    #\n",
      "    # Put the result into a color plot\n",
      "    Z = Z.reshape(xx.shape)\n",
      "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
      "    plt.axis('off')\n",
      "    #\n",
      "    # Plot also the training points\n",
      "    plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n",
      "    #\n",
      "    plt.title(titles[i])\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classwork : Playing with Gamma, Cost, and Kernels"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Practice changing the values for each in the set_params() function. Remember to refit your model anytime you change parameters."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compare the performance before and after."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Did shrinking cost improve the generalization error as expected? Did increasing it improve the accuracy of the model?\n",
      "2. How does changing gamma in the rbf kernel effect your results?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "SVR Implementation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Support Vector Machines can also be used for regression problems! Let's compare the results of a couple kernels on the original mammals data set. Compare the results of each fit afterwords with adjusted r-squared and MSE."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import DataFrame, read_csv\n",
      "%matplotlib inline \n",
      "\n",
      "mammals = read_csv('http://bit.ly/1f2YPsC').sort('body')\n",
      "lm = svm.SVR(kernel='linear', C=1e1)\n",
      "lm_rbf = svm.SVR(kernel='rbf', C=1e1)\n",
      "\n",
      "body = mammals[ ['body'] ].values\n",
      "brain = mammals.brain.values\n",
      "\n",
      "lm.fit(body, brain)\n",
      "lm_rbf.fit(np.log(body), np.log(brain))\n",
      "\n",
      "## Compare to the original log fit model, as well as other svm kernels:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "logfit = LinearRegression().fit(np.log(body), np.log(brain))\n",
      "mammals['log_regr'] = np.exp(logfit.predict(np.log(body)))\n",
      "mammals['linear_svm'] = lm.predict(body)\n",
      "mammals['rbf_svm'] = np.exp(lm_rbf.predict(np.log(body)))\n",
      "\n",
      "plt.scatter(body, brain)\n",
      "plt.plot(body, mammals['linear_svm'].values, c='r', label='linear svm')\n",
      "plt.plot(body, mammals['rbf_svm'].values, c='g', label='gaussian svm')\n",
      "plt.plot(body, mammals['log_regr'].values, c='b', label='linear regression')\n",
      "plt.xlabel('data')\n",
      "plt.ylabel('target')\n",
      "plt.title('Support Vector Regression')\n",
      "plt.legend(loc=2)\n",
      "plt.show()\n",
      "\n",
      "for prediction in ('linear_svm', 'rbf_svm', 'log_regr'):\n",
      "\tprint 'Mean Squared Error for', prediction, ':', metrics.mean_squared_error(mammals[ [prediction] ].values, mammals[ ['brain'] ].values)\n",
      "\tprint 'R-Squared for', prediction, ':', metrics.r2_score(mammals[ [prediction] ].values, mammals[ ['brain'] ].values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classwork"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Try toying with SVRs for the baseball regression problem (this should mostly just be a dropin replacement). How does this change your performance? Would any kernel make sense to use here?\n",
      "2. Try also using SVMs for classifying the \"IsBadBuy\" in the classification exercise. What changes can you make to make this work better?\n",
      "\n",
      "Continue working on your two ongoing assignments after these two implementations."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/resources.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Resources"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Academic"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Stanford CS229 Lecture notes on SVM](http://cs229.stanford.edu/notes/cs229-notes3.pdf)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Video Lectures"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Support Vector Machines](http://videolectures.net/mlss06tw_lin_svm/)\n",
      "* [Deep Support Vector Machines](http://videolectures.net/roks2013_wiering_vector/)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Packages"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Support Vector Machines](http://scikit-learn.org/stable/modules/svm.html#svm-kernels)\n",
      "* [Kernel Functions](http://scikit-learn.org/stable/modules/svm.html#svm-kernels)\n",
      "* [sklearn.svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
      "* [sklearn.svm.LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
      "* [sklearn.svm.SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
      "* [PyML SVM Howto](http://pyml.sourceforge.net/doc/howto.pdf)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}