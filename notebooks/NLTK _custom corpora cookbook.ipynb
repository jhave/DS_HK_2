{
 "metadata": {
  "name": "",
  "signature": "sha256:fa0407f597bf110e1d94886436ac32d74d654887c75f6356a83529aecdf77c3a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "working thru:\n",
      "https://www.packtpub.com/article/python-text-processing-nltk-20-creating-custom-corpora"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os, os.path\n",
      "path = os.path.expanduser('~/nltk_data')\n",
      "if not os.path.exists(path):\n",
      "    os.mkdir(path)\n",
      "os.path.exists(path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk.data\n",
      "path in nltk.data.path"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path = os.path.expanduser('~/nltk_data/corpora')\n",
      "if not os.path.exists(path):\n",
      "    os.mkdir(path)\n",
      "os.path.exists(path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path = os.path.expanduser('~/nltk_data/corpora/cookbook')\n",
      "if not os.path.exists(path):\n",
      "    os.mkdir(path)\n",
      "os.path.exists(path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test load\n",
      "nltk.data.load('corpora/cookbook/mywords.txt', format='raw')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "'nltk\\n'"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# loading a YAML file\n",
      "nltk.data.load('corpora/cookbook/synonyms.yaml')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "{'bday': 'birthday'}"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#using simple corpus reader class\n",
      "from nltk.corpus.reader import WordListCorpusReader \n",
      "reader = WordListCorpusReader(r\"C:\\Users\\david\\nltk_data\\corpora\\cookbook\", ['wordlist.txt'])\n",
      "reader.words()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "['nltk', 'corpus', 'corpora', 'wordnet']"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# raw output\n",
      "reader.raw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "'nltk\\r\\ncorpus\\r\\ncorpora\\r\\nwordnet'"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize it\n",
      "from nltk.tokenize import line_tokenize\n",
      "line_tokenize(reader.raw())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "['nltk is not so extremely difficult',\n",
        " 'corpure core pure',\n",
        " 'corpora core pore',\n",
        " 'wordnet weird net']"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# name corpus might prove useful generating fake bios\n",
      "from nltk.corpus import names\n",
      "names.fileids()\n",
      "['female.txt', 'male.txt']\n",
      "len(names.words('female.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "5001"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(names.words('male.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "2943"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#NLTK also comes with a large list of English words. \n",
      "#There's one file with 850 basic words, \n",
      "#and another list with over 200,000 known English words.\n",
      "from nltk.corpus import words\n",
      "words.fileids()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "['en', 'en-basic']"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(words.words('en-basic'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "850"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(words.words('en'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "235886"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# simple tagged corpus \"word/tag\"\n",
      "\n",
      "# using an excerpt of brown corpus stored in file called brown.pos\n",
      "# The/at-tl expense/nn and/cc time/nn involved/vbn are/ber astronomical/ jj ./.\n",
      "\n",
      "\"\"\" IMPORTANT : use a regular expression, r'.*\\.pos', to match all files whose name ends with .pos. \n",
      "We could have done the same thing as we did with the WordListCorpusReader, and pass ['brown.pos'] as the second argument, \n",
      "but this way you can see how to include multiple files in a corpus without naming each one explicitly.\n",
      "\"\"\"\n",
      "\n",
      "from nltk.corpus.reader import TaggedCorpusReader\n",
      "reader = TaggedCorpusReader(r\"C:\\Users\\david\\nltk_data\\corpora\\cookbook\", r'.*\\.pos')\n",
      "reader.words()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "['The', 'expense', 'and', 'time', 'involved', 'are', ...]"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reader.tagged_words()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "[('The', 'AT-TL'), ('expense', 'NN'), ('and', 'CC'), ...]"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reader.sents()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "[['The', 'expense', 'and', 'time', 'involved', 'are', 'astronomical', 'jj', '.']]"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reader.tagged_sents()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "[[('The', 'AT-TL'), ('expense', 'NN'), ('and', 'CC'), ('time', 'NN'), ('involved', 'VBN'), ('are', 'BER'), ('astronomical', ''), ('jj', None), ('.', '.')]]"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reader.paras()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "[[['The', 'expense', 'and', 'time', 'involved', 'are', 'astronomical', 'jj', '.']]]"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reader.tagged_paras()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "[[[('The', 'AT-TL'), ('expense', 'NN'), ('and', 'CC'), ('time', 'NN'), ('involved', 'VBN'), ('are', 'BER'), ('astronomical', ''), ('jj', None), ('.', '.')]]]"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"IMPORTANT: ... cld b used for Verse and line breaks...\n",
      "Customizing the paragraph block reader\n",
      "Paragraphs are assumed to be split by blank lines. This is done with the default para_ block_reader, which is nltk.corpus.reader.util.read_blankline_block. There are a number of other block reader functions in nltk.corpus.reader.util, whose purpose is to read blocks of text from a stream. Their usage will be covered in more detail in the later recipe, Creating a custom corpus view, where we'll create a custom corpus reader.\n",
      "\"\"\"\n",
      "\n",
      "reader = TaggedCorpusReader(r\"C:\\Users\\david\\nltk_data\\corpora\\cookbook\", r'.*\\.pos', tag_mapping_function=lambda t: t.lower())\n",
      "reader.tagged_words(simplify_tags=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "<repr(<nltk.corpus.reader.tagged.TaggedCorpusView at 0x7d451d0>) failed: AttributeError: 'NoneType' object has no attribute 'lower'>"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tag import simplify\n",
      "reader = TaggedCorpusReader('.', r'.*\\.pos', tag_mapping_\n",
      "function=simplify.simplify_brown_tag)\n",
      "reader.tagged_words(simplify_tags=True)\n",
      "[('The', 'DET'), ('expense', 'N'), ('and', 'CNJ'), ...]\n",
      "reader = TaggedCorpusReader(r\"C:\\Users\\david\\nltk_data\\corpora\\cookbook\", r'.*\\.pos', tag_mapping_function=simplify.simplify_tag)\n",
      "reader.tagged_words(simplify_tags=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-49-5c49e9aa26de>, line 3)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-49-5c49e9aa26de>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    function=simplify.simplify_brown_tag)\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " from nltk.corpus.reader import ChunkedCorpusReader\n",
      "reader = ChunkedCorpusReader(r\"C:\\Users\\david\\nltk_data\\corpora\\cookbook\",  r'.*\\.chunk')\n",
      "reader.chunked_words()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "[Tree('NP', [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]), ('have', 'VBP'), ...]"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# An alternative format for denoting chunks is called IOB tags.\n",
      "# To read a corpus using the IOB format, you must use the ConllChunkCorpusReader.\n",
      "\n",
      "# B-NP denotes the beginning of a noun phrase, \n",
      "# while I-NP denotes that the word is inside of the current noun phrase. \n",
      "# B-VP and I-VP denote the beginning and inside of a verb phrase. \n",
      "# O ends the sentence.\n",
      "\n",
      "from nltk.corpus.reader import ConllChunkCorpusReader\n",
      "conllreader = ConllChunkCorpusReader(r\"C:\\Users\\david\\nltk_data\\corpora\\cookbook\", r'.*\\.iob', ('NP', \n",
      "'VP', 'PP'))\n",
      "conllreader.chunked_words()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "[Tree('NP', [('Mr.', 'NNP'), ('Meador', 'NNP')]), Tree('VP', [('had', 'VBD'), ('been', 'VBN')]), ...]"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conllreader.chunked_sents()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "[Tree('S', [Tree('NP', [('Mr.', 'NNP'), ('Meador', 'NNP')]), Tree('VP', [('had', 'VBD'), ('been', 'VBN')]), Tree('NP', [('executive', 'JJ'), ('vice', 'NN'), ('president', 'NN')]), Tree('PP', [('of', 'IN')]), Tree('NP', [('Balcor', 'NNP')]), ('.', '.')])]"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conllreader.iob_sents()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "[[('Mr.', 'NNP', 'B-NP'), ('Meador', 'NNP', 'I-NP'), ('had', 'VBD', 'B-VP'), ('been', 'VBN', 'I-VP'), ('executive', 'JJ', 'B-NP'), ('vice', 'NN', 'I-NP'), ('president', 'NN', 'I-NP'), ('of', 'IN', 'B-PP'), ('Balcor', 'NNP', 'B-NP'), ('.', '.', 'O')]]"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#  to get a list of all the tagged tokens in a tree, call the leaves() method.\n",
      "reader.chunked_words()[0].leaves()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 59,
       "text": [
        "[('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      " find out more at http://www.cis.upenn.edu/~treebank/home.html.\n",
      "CoNLL stands for the Conference on Computational Natural Language Learning.  read more at http://www.cnts.ua.ac.be/conll2000/chunking/."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Creating a categorized text corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import brown\n",
      "brown.categories()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 60,
       "text": [
        "['adventure',\n",
        " 'belles_lettres',\n",
        " 'editorial',\n",
        " 'fiction',\n",
        " 'government',\n",
        " 'hobbies',\n",
        " 'humor',\n",
        " 'learned',\n",
        " 'lore',\n",
        " 'mystery',\n",
        " 'news',\n",
        " 'religion',\n",
        " 'reviews',\n",
        " 'romance',\n",
        " 'science_fiction']"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The first two arguments to CategorizedPlaintextCorpusReader are the root directory and fileids, which are passed on to the PlaintextCorpusReader to read n the files. The cat_pattern keyword argument is a regular expression for extracting the category names from the fileids. In our case, the category is the part of the fileid after movie_ and before .txt. \n",
      "\n",
      "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
      "reader = CategorizedPlaintextCorpusReader(r\"C:\\Users\\david\\nltk_data\\corpora\\cookbook\", r'movie_.*\\.txt', cat_pattern=r'movie_(\\w+)\\.txt')\n",
      "reader.categories()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 66,
       "text": [
        "['neg', 'pos']"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reader.categories()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 67,
       "text": [
        "['neg', 'pos']"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reader.fileids(categories=['neg'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 68,
       "text": [
        "['movie_neg.txt']"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reader.fileids(categories=['pos'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 69,
       "text": [
        "['movie_pos.txt']"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "IMPORTANT & USEFUL FOR PRE-LABELLED POEMS: Instead of cat_pattern, you could pass in a cat_map, which is a dictionary mapping a fileid to a list of category labels. --- BUT IT WOULD BE BETTER IF WE COULD SEARCH AND EXTRACT POEMS WITH THAT LABEL FROM HTML CORPUS... HOW?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reader = CategorizedPlaintextCorpusReader(r\"C:\\Users\\david\\nltk_data\\corpora\\cookbook\", r'movie_.*\\.txt', cat_map={'movie_pos.txt': ['pos'], 'movie_neg.txt': \n",
      "['neg']})\n",
      "reader.categories()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 72,
       "text": [
        "['neg', 'pos']"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Creating a categorized chunk corpus reader"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#  a class called CategorizedChunkedCorpusReader that inherits from both CategorizedCorpusReader and ChunkedCorpusReader\n",
      "\n",
      "from nltk.corpus.reader import CategorizedCorpusReader, ChunkedCorpusReader\n",
      "class CategorizedChunkedCorpusReader(CategorizedCorpusReader, ChunkedCorpusReader):\n",
      "  def __init__(self, *args, **kwargs):\n",
      "    CategorizedCorpusReader.__init__(self, kwargs)\n",
      "    ChunkedCorpusReader.__init__(self, *args, **kwargs)\n",
      "  def _resolve(self, fileids, categories):\n",
      "    if fileids is not None and categories is not None:\n",
      "      raise ValueError('Specify fileids or categories, not both')\n",
      "    if categories is not None:\n",
      "      return self.fileids(categories)\n",
      "    else:\n",
      "      return fileids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CategorizedChunkedCorpusReader overrides all the ChunkedCorpusReader methods to take a categories argument for locating fileids. These fileids are found with the internal _resolve() function. This _resolve() function makes use of CategorizedCorpusReader.fileids() to return fileids for a given list of categories. If no categories are given, _resolve() just returns the given fileids, which could be None, in which case all files are read. The initialization of both CategorizedCorpusReader and ChunkedCorpusReader is what makes this all possible. If you look at the code for CategorizedTaggedCorpusReader, you'll see it's very similar."
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "OVERIDING the classes concerning paragraphs and sentences will be necessary to deal with poetry which has lines at line-breaks and verses at double-line-breaks. -- see article for more...."
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "IMPORTANT: Corpus Views, read blocks of text (poems) into stream buffer and apply custom functions to them -- also investigate pickle"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OR USE: read_blankline_block() will read lines from the stream until it finds a blank line. It will then return a single token of all lines found combined into a single string."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}